#fundamentals

The ability to explain or to present an ML [[model|model&#39;s]] reasoning in
understandable terms to a human.

Most [[linear regression|linear regression]] models, for example, are highly
interpretable. (You merely need to look at the trained weights for each
feature.) Decision forests are also highly interpretable. Some models, however,
require sophisticated visualization to become interpretable.

You can use the
[[Learning Interpretability Tool (LIT)|Learning Interpretability Tool (LIT)]]
to interpret ML models.

