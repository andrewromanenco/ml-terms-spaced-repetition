#language

A mechanism used in a [[neural network|neural network]] that indicates
the importance of a particular word or part of a word. Attention compresses
the amount of information a model needs to predict the next token/word.
A typical attention mechanism might consist of a
[[weighted sum|weighted sum]] over a set of inputs, where the
[[weight|weight]] for each input is computed by another part of the
neural network.

Refer also to [[self-attention (also called self-attention layer)|self-attention]] and
[[multi-head self-attention|multi-head self-attention]], which are the
building blocks of [[Transformer|Transformers]].

