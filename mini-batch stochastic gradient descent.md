
A [[gradient descent|gradient descent]] algorithm that uses
[[mini-batch|mini-batches]]. In other words, mini-batch stochastic
gradient descent estimates the gradient based on a small subset of the
training data. Regular [[stochastic gradient descent (SGD)|stochastic gradient descent]] uses a
mini-batch of size 1.

