#fundamentals

A state reached when [[loss|loss]] values change very little or
not at all with each [[iteration|iteration]]. For example, the following
[[loss curve|loss curve]] suggests convergence at around 700 iterations:


![[ images/Convergence.png ]]


A model <strong>converges</strong> when additional training won&#39;t
improve the model.

In [[deep model|deep learning]], loss values sometimes stay constant or
nearly so for many iterations before finally descending. During a long period
of constant loss values, you may temporarily get a false sense of convergence.

See also [[early stopping|early stopping]].

