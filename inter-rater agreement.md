
A measurement of how often human raters agree when doing a task.
If raters disagree, the task instructions may need to be improved.
Also sometimes called <strong>inter-annotator agreement</strong> or
<strong>inter-rater reliability</strong>. See also
<a href="https://wikipedia.org/wiki/Cohen%27s_kappa" target="T">Cohen&#39;s
kappa</a>,
which is one of the most popular inter-rater agreement measurements.

