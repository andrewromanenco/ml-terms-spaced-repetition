
A specific implementation of the [[gradient descent|gradient descent]]
algorithm. Popular optimizers include:

<ul>
<li>[[AdaGrad|AdaGrad]], which stands for ADAptive GRADient descent.</li>
<li>Adam, which stands for ADAptive with Momentum.</li>
</ul>

