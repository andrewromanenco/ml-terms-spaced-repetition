#language

An extension of [[self-attention (also called self-attention layer)|self-attention]] that applies the
self-attention mechanism multiple times for each position in the input sequence.

[[Transformer|Transformers]] introduced multi-head self-attention.

