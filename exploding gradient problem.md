#seq

The tendency for [[gradient|gradients]] in
[[deep neural network|deep neural networks]] (especially
[[recurrent neural network|recurrent neural networks]]) to become
surprisingly steep (high). Steep gradients often cause very large updates
to the [[weight|weights]] of each [[node (neural network)|node]] in a
deep neural network.

Models suffering from the exploding gradient problem become difficult
or impossible to train. [[gradient clipping|Gradient clipping]]
can mitigate this problem.

Compare to [[vanishing gradient problem|vanishing gradient problem]].


