#fundamentals

A [[gradient descent|gradient descent]] algorithm in which the
[[batch size|batch size]] is one. In other words, SGD trains on
a single example chosen uniformly at
random from a [[training set|training set]].

