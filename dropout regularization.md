
A form of [[regularization|regularization]] useful in training
[[neural network|neural networks]]. Dropout regularization
removes a random selection of a fixed number of the units in a network
layer for a single gradient step. The more units dropped out, the stronger
the regularization. This is analogous to training the network to emulate
an exponentially large [[ensemble|ensemble]] of smaller networks.
For full details, see
<a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"
target="T">Dropout: A Simple Way to Prevent Neural Networks from
Overfitting</a>.

